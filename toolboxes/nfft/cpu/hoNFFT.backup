// Includes - Thrust
//#include <thrust/scan.h>
//#include <thrust/sort.h>
//#include <thrust/binary_search.h>
//#include <thrust/extrema.h>
// Includes - Gadgetron
#include "hoNFFT.h"
#include "hoNDFFT.h"
//#include "cuNDArray_operators.h"
#include "hoNDArray_elemwise.h"
#include "hoNDArray_utils.h"
#include "vector_td_utilities.h"
#include "vector_td_io.h"
//#include "cudaDeviceManager.h"
//#include "check_CUDA.h"

// Includes - CUDA
//#include <device_functions.h>
//#include <math_constants.h>
//#include <cufft.h>


// Includes - stdlibs
#include <stdio.h>
#include <assert.h>
#include <limits.h>
#include <math.h>
#include <cmath>
#include <sstream>
#include <stdexcept>

//using namespace std;
using std::vector;
//using namespace thrust;
using namespace Gadgetron;

// Kernel configuration  
#define NFFT_MAX_COILS_COMPUTE_1x    8
#define NFFT_MAX_COILS_COMPUTE_2x   16
#define NFFT_THREADS_PER_KERNEL    192

// Reference to shared memory
//extern __shared__ char _shared_mem[];

// Includes containing the NFFT convolution implementation
#include "KaiserBessel_operators.h"
//#include "NFFT_C2NC_conv_kernel.cu"
//#include "NFFT_NC2C_conv_kernel.cu"
//#include "NFFT_NC2C_atomic_conv_kernel.cu"
#include "NFFT_preprocess.cpp"


// Default template arguments requires c++-0x ?
typedef float dummy;


// The declaration of atomic/non-atomic NC2C convolution
// We would love to hide this inside the class, but the compiler core dumps on us when we try...
//

/**
template<class T, unsigned int D> struct _convolve_NFFT_NC2C{
  static bool apply( hoNFFT_plan<float,D> *plan, 
                     hoNDArray<std::complex<T> > *in, 
                     hoNDArray<std::complex<T> > *out, 
                     bool accumulate );
};

// Common multi-device handling: prepare
//
template<class I1, class I2, class I3>
static bool prepare( int device, int *old_device, 
                     cuNDArray<I1> *in1,       cuNDArray<I1> **in1_int,
                     cuNDArray<I2> *in2 = 0x0, cuNDArray<I2> **in2_int = 0x0,
                     cuNDArray<I3> *in3 = 0x0, cuNDArray<I3> **in3_int = 0x0 )
{
  // Get current Cuda device
  if( cudaGetDevice(old_device) != cudaSuccess ) {
    throw cuda_error("Error: cuNFFT : unable to get device no");
  }

  if( device != *old_device && cudaSetDevice(device) != cudaSuccess) {
    throw cuda_error("Error : cuNFFT : unable to set device no");
  }
  
  // Transfer arrays to compute device if necessary
  if( in1 ){
    if( device != in1->get_device() )
      *in1_int = new cuNDArray<I1>(*in1); // device transfer
    else
      *in1_int = in1;
  }
  
  if( in2 ){
    if( device != in2->get_device() )
      *in2_int = new cuNDArray<I2>(*in2); // device transfer
    else
      *in2_int = in2;
  }

  if( in3 ){
    if( device != in3->get_device() )
      *in3_int = new cuNDArray<I3>(*in3); // device transfer
    else
      *in3_int = in3;
  }
  
  return true;
}  

// Common multi-device handling: restore
//
template<class I1, class I2, class I3>
static bool restore( int old_device, cuNDArray<I1> *out, 
                     cuNDArray<I1> *in1, cuNDArray<I1> *in1_int,
                     cuNDArray<I2> *in2 = 0x0, cuNDArray<I2> *in2_int = 0x0,
                     cuNDArray<I3> *in3 = 0x0, cuNDArray<I3> *in3_int = 0x0 )
{
  if( in1 && out && out->get_device() != in1_int->get_device() ){ 
    *out = *in1_int; // device transfer by assignment
  } 
  
  // Check if internal array needs deletion (they do only if they were created in ::prepare()
  //
  if( in1 && in1->get_device() != in1_int->get_device() ){
    delete in1_int;
  }   
  if( in2 && in2->get_device() != in2_int->get_device() ){
    delete in2_int;
  }   
  if( in3 && in3->get_device() != in3_int->get_device() ){
    delete in3_int;
  }   
  
  // Get current Cuda device
  int device;
  if( cudaGetDevice(&device) != cudaSuccess ) {
    throw cuda_error("Error: cuNFFT : unable to get device no");
  }
  
  // Restore old device
  if( device != old_device && cudaSetDevice(old_device) != cudaSuccess) {
    throw cuda_error("Error: cuNFFT : unable to restore device no");
  }
  
  return true;
}

*/

//
// Public class methods
//

	   

template<class T, unsigned int D> Gadgetron::hoNFFT_plan<T,D>::hoNFFT_plan()
{
  // Minimal initialization
  barebones();
}

template<class T, unsigned int D> Gadgetron::hoNFFT_plan<T,D>::hoNFFT_plan( typename uint64d<D>::Type matrix_size, typename uint64d<D>::Type matrix_size_os, T W )
{
    // Minimal initialization
    barebones();

    // Setup plan
    setup( matrix_size, matrix_size_os, W );
}

template<class T, unsigned int D> Gadgetron::hoNFFT_plan<T,D>::~hoNFFT_plan()
{
    wipe(NFFT_WIPE_ALL);
}


template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::setup( typename uint64d<D>::Type matrix_size, typename uint64d<D>::Type matrix_size_os, T W )
{
    // Free memory
    wipe(NFFT_WIPE_ALL);

    // The convolution does not work properly for very small convolution kernel widths
    // (experimentally observed limit)

    if( W < T(1.8) ) {
	throw std::runtime_error("Error: the convolution kernel width for the cuNFFT plan is too small.");
    }

    //
    // Check input against certain requirements
    //
    
    //
    // Setup private variables
    //

    this->matrix_size = matrix_size;
    this->matrix_size_os = matrix_size_os;
    
    T W_half = T(0.5)*W; //Half kernel size
    vector_td<T,D> W_vec(W_half); //Vector of half kernel size repeated D times

    matrix_size_wrap = vector_td<size_t,D>( ceil(W_vec) );
    //This is used for increasing the matrix_size_os on all sides by the kernel size when convolving with the kernel
    matrix_size_wrap<<=1; //matrix_wrap_size = matrix_wrap_size << 1 = matrix_wrap_size * 2 
    alpha = vector_td<T,D>(matrix_size_os) / vector_td<T,D>(matrix_size);
  
    typename reald<T,D>::Type ones(T(1));
    if( weak_less( alpha, ones ) ){
	throw std::runtime_error("Error: hoNFFT : Illegal oversampling ratio suggested");
    }

    this->W = W;
  
    // Compute Kaiser-Bessel beta
    compute_beta();
  
    initialized = true;
}


template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::preprocess( hoNDArray<typename reald<T,D>::Type> *trajectory, NFFT_prep_mode mode )
{
    if( !trajectory || trajectory->get_number_of_elements()==0 ){
	throw std::runtime_error("Error: hoNFFT_plan::preprocess: invalid trajectory");
    }
    
    if( !initialized ){
	throw std::runtime_error("Error: hoNFFT_plan::preprocess: hoNFFT_plan::setup must be invoked prior to preprocessing.");
    }
    
    wipe(NFFT_WIPE_PREPROCESSING);
    
    //hoNDArray<typename reald<T,D>::Type> *trajectory_int;
    hoNDArray<T> *trajectory_int;    
    
    number_of_samples = trajectory->get_size(0);
    number_of_frames = trajectory->get_number_of_elements()/number_of_samples;
    
    // Make sure that the trajectory values are within range [-1/2;1/2]
    std::pair<T*,T*> mm_pair = std::minmax_element(trajectory_int->get_data_ptr(),  (trajectory_int->get_data_ptr()+trajectory_int->get_number_of_elements()*D));
  
    if( *mm_pair.first < T(-0.5) || *mm_pair.second > T(0.5) ){
    	std::stringstream ss;
    	ss << "Error: hoNFFT::preprocess : trajectory [" << *mm_pair.first << "; " << *mm_pair.second << "] out of range [-1/2;1/2]";
    	throw std::runtime_error(ss.str());
    }
  
    // Make Thrust device vector of trajectory and samples
    //vector_td<T,D>  trajectory_positions( vector_td<T,D> (trajectory_int->get_data_ptr()), vector_td<T,D> (trajectory_int->get_data_ptr()+trajectory_int->get_number_of_elements() ));
  
   
    //trajectory_positions = new  vector_td<T,D> ( trajectory->get_number_of_elements() );
    trajectory_positions->create(trajectory->get_dimensions(), trajectory->get_data_ptr());
    //(  (trajectory->get_data_ptr()), (trajectory->get_data_ptr()+trajectory->get_number_of_elements() ) );

    

    // ####### CALC TRAJECTORY POSITIONS #######
    
    
    vector_td<T,D> matrix_size_os_real = vector_td<T,D>( matrix_size_os ); //Making vector sized D with all elements having the value of matrix_size_os
    vector_td<T,D> matrix_size_os_plus_wrap_real = vector_td<T,D>( (matrix_size_os+matrix_size_wrap)>>1 ); //Added kernel width and taken half
    
    
    // convert input trajectory in [-1/2;1/2] to [0;matrix_size_os]
    /**
    thrust::transform( trajectory_positions_in.begin(), 
		       trajectory_positions_in.end(), 
		       trajectory_positions->begin(), 
    		       trajectory_scale<REAL,D>(matrix_size_os_real, matrix_size_os_plus_wrap_real) );

    */
    
    
    // Make trajectory postions in range [0 (matrix_size_os + matrix_size_wrap)] instead of [-1/2 1/2]
    for(int i = 0; i < trajectory_positions->get_number_of_elements()/D; i++){
	trajectory_positions->get_data_ptr()[i*D  ] = ( trajectory_positions->get_data_ptr()[i*D  ] + T(0.5) ) * T(matrix_size_os[0]+matrix_size_wrap[0]); //Wrap size??
	if (D > 1)
	    trajectory_positions->get_data_ptr()[i*D+1] = ( trajectory_positions->get_data_ptr()[i*D+1] + T(0.5) ) * T(matrix_size_os[1]+matrix_size_wrap[1]); //Wrap size??
	if (D > 2)
	    trajectory_positions->get_data_ptr()[i*D+2] = ( trajectory_positions->get_data_ptr()[i*D+2] + T(0.5) ) * T(matrix_size_os[2]+matrix_size_wrap[2]); //Wrap size??
	if (D > 3)
	    trajectory_positions->get_data_ptr()[i*D+3] = ( trajectory_positions->get_data_ptr()[i*D+3] + T(0.5) ) * T(matrix_size_os[3]+matrix_size_wrap[3]); //Wrap size??
    }
    
    
    
    
    if( !( mode == NFFT_PREP_C2NC )){  //NC2C and not ATOMICS
	

	T half_W = T(0.5)*W;
	
	//Calculating number of cells for the convolution kernel to cover
	unsigned int upper_limit = (unsigned int) std::floor( half_W );
	unsigned int lower_limit = (unsigned int) std::ceil( -half_W );
	unsigned int num_cells = D*(upper_limit-lower_limit+1);

        grid_cell_pos.create(trajectory_positions->get_number_of_elements()*num_cells);
        traj_idx.create(trajectory_positions->get_number_of_elements()*num_cells);

	int traj_length = trajectory_positions->get_number_of_elements();
	
	output_pairs<T>( trajectory_positions->get_data_ptr(), traj_length, vector_td<unsigned int,D> (matrix_size_os), vector_td<unsigned int,D> (matrix_size_wrap), 
			 grid_cell_pos.get_data_ptr(), traj_idx.get_data_ptr(), half_W, num_cells );

	/**
	// allocate storage for and compute temporary prefix-sum variable (#cells influenced per sample)
	unsigned int c_p_s(trajectory_int->get_number_of_elements());
	unsigned int c_p_s_ps(trajectory_int->get_number_of_elements());
	
        
	thrust::plus<unsigned int> binary_op;
	thrust::transform(trajectory_positions->begin(), trajectory_positions->end(), c_p_s.begin(), compute_num_cells_per_sample<REAL,D>(half_W));
	inclusive_scan( c_p_s.begin(), c_p_s.end(), c_p_s_ps.begin(), binary_op ); // prefix sum
	
	// Build the vector of (grid_idx, sample_idx) tuples. Actually kept in two seperate vectors.
	unsigned int num_pairs = c_p_s_ps.back();
	c_p_s.clear();
	
	unsigned int tuples_first = new unsigned int trajectory->get_number_of_elements(); //(num_pairs);
	tuples_last = new unsigned int (num_pairs);
	
	
	// Fill tuple vector
	write_pairs<T,D>( vector_td<unsigned int,D>(matrix_size_os), 
			  vector_td<unsigned int,D>(matrix_size_wrap), 
			  number_of_samples, 
			  number_of_frames, 
			  W,
			  trajectory_positions[0], 
			  c_p_s_ps[0], //Write offsets 
			  tuples_first[0], 
			  tuples_last[0] );
	
	c_p_s_ps.clear();
	
	
	// Sort by grid indices
	sort_by_key( tuples_first->begin(), tuples_first->end(), tuples_last->begin() );
	
	// each bucket_begin[i] indexes the first element of bucket i's list of points
	// each bucket_end[i] indexes one past the last element of bucket i's list of points
	
	bucket_begin = new device_vector<unsigned int>(number_of_frames*prod(matrix_size_os+matrix_size_wrap));
	bucket_end   = new device_vector<unsigned int>(number_of_frames*prod(matrix_size_os+matrix_size_wrap));
	
	
	
	// find the beginning of each bucket's list of points
	counting_iterator<unsigned int> search_begin(0);
	lower_bound(tuples_first->begin(), tuples_first->end(), search_begin, search_begin + number_of_frames*prod(matrix_size_os+matrix_size_wrap), bucket_begin->begin() );
	
	// find the end of each bucket's list of points
	upper_bound(tuples_first->begin(), tuples_first->end(), search_begin, search_begin + number_of_frames*prod(matrix_size_os+matrix_size_wrap), bucket_end->begin() );
	
	delete tuples_first;
	*/
    }

    
    
    preprocessed_C2NC = true;
    
    if( mode != NFFT_PREP_C2NC )
	preprocessed_NC2C = true;
        
}



template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::compute( hoNDArray<std::complex<T> > *in, hoNDArray<std::complex<T> > *out,
									     hoNDArray<T> *dcw, NFFT_comp_mode mode )
{  
    // Validity checks
    
    unsigned char components;
    
    if( mode == NFFT_FORWARDS_C2NC ) 
	components = _NFFT_CONV_C2NC + _NFFT_FFT + _NFFT_DEAPODIZATION;
    
    else if( mode == NFFT_FORWARDS_NC2C ) 
	components = _NFFT_CONV_NC2C + _NFFT_FFT + _NFFT_DEAPODIZATION;
    
    else if( mode == NFFT_BACKWARDS_NC2C ) 
	components = _NFFT_CONV_NC2C + _NFFT_FFT + _NFFT_DEAPODIZATION;
    
    else if( mode == NFFT_BACKWARDS_C2NC ) 
	components = _NFFT_CONV_C2NC + _NFFT_FFT + _NFFT_DEAPODIZATION;
    else{
	throw std::runtime_error("Error: hoNFFT_plan::compute: unknown mode");
    }

  
    
    {
	hoNDArray<std::complex<T> > *samples, *image;
	
	if( mode == NFFT_FORWARDS_C2NC || mode == NFFT_BACKWARDS_C2NC ){
	    image = in; samples = out;
	} else{
	    image = out; samples = in;
	}
	
	check_consistency( samples, image, dcw, components );
    }
    
    hoNDArray<std::complex<T> > *in_int = 0x0, *out_int = 0x0;
    hoNDArray<T> *dcw_int = 0x0;
    //int old_device;
    
    typename uint64d<D>::Type image_dims = from_std_vector<size_t,D>
	( (mode == NFFT_FORWARDS_C2NC || mode == NFFT_BACKWARDS_C2NC ) ? *in->get_dimensions() : *out->get_dimensions() ); //Must be D-dimensional integer (integer representing D sizes)
    bool oversampled_image = (image_dims==matrix_size_os);
    
    vector<size_t> vec_dims = to_std_vector(matrix_size_os); //Must be 1-d standard vector with D number of elements
    {
	hoNDArray<std::complex<T> > *image = ((mode == NFFT_FORWARDS_C2NC || mode == NFFT_BACKWARDS_C2NC ) ? in : out );
	for( unsigned int d=D; d<image->get_number_of_dimensions(); d++ )
	    vec_dims.push_back(image->get_size(d));
    }
    
    hoNDArray<std::complex<T> > *working_image = 0x0;
    hoNDArray<std::complex<T> > *working_samples = 0x0;
    
    switch(mode){
	
    case NFFT_FORWARDS_C2NC:
	
	if( !oversampled_image ){
	    //typename uint64d<D>::Type& matrix_size_os_tmp = matrix_size_os;
	    working_image = new hoNDArray<std::complex<T> >(&vec_dims);

	    /**
	    std::vector<size_t> kh_dims(4);
	    kh_dims[0] = 10;
	    kh_dims[1] = 10;
	    kh_dims[2] = 10;
	    kh_dims[3] = 10;
	    hoNDArray<float> kh_A(kh_dims); 
	    hoNDArray<float> kh_B;
	    std::vector<size_t> B_dims(4); 
	    B_dims[0] = 20;
	    B_dims[1] = 20;
	    B_dims[2] = 20;
	    B_dims[3] = 20;
	    
	    //dims[0] = 20;
	    //dims[1] = 20;
	    //dims[2] = 20;
	    //dims[3] = 20;

	    hoNDArray<float> * Ap = &kh_A;
	    hoNDArray<float> * Bp = &kh_B;
	    
	    typename uint64d<D>::Type dims = from_std_vector<size_t,D>(*Bp->get_dimensions()) ;

	    //hoNDArray<float> kh_A = new hoNDArray<float>(kh_dims);
	    pad(20, &kh_A, &kh_B);
	    */

	    pad(matrix_size_os[0], in_int, working_image );
	}
	else{
	    working_image = in_int;
	}
	
	compute_NFFT_C2NC( working_image, out_int );
	
	
	if( dcw_int )
	    multiply(*out_int, *dcw_int, *out_int);
	
	
	if( !oversampled_image ){
	    delete working_image; working_image = 0;
	}    
	break;
	
	
    case NFFT_FORWARDS_NC2C:
	
	// Density compensation
	if( dcw_int ){
	    working_samples = new hoNDArray<std::complex<T> >(*in_int);
	    multiply(*working_samples, *dcw_int, *working_samples);
	}
	else{
	    working_samples = in_int;
	}
	
	if( !oversampled_image ){
	    working_image = new hoNDArray<std::complex<T> >(&vec_dims);
	}
	else{
	    working_image = out_int;
	}
    
	compute_NFFT_NC2C( working_samples, working_image );
	
	if( !oversampled_image ){
	    crop<std::complex<T>, D>( (matrix_size_os-matrix_size)>>1, working_image, out_int );
	}
	
	if( !oversampled_image ){
	    delete working_image; working_image = 0x0;
	}
	
	if( dcw_int ){
	    delete working_samples; working_samples = 0x0;
	}    
	break;
	
    case NFFT_BACKWARDS_NC2C:
	
	// Density compensation
	if( dcw_int ){
	    working_samples = new hoNDArray<std::complex<T> >(*in_int);
	    *working_samples *= *dcw_int;
	}
	else{
	    working_samples = in_int;
	}
	
	if( !oversampled_image ){
	    working_image = new hoNDArray<std::complex<T> >(&vec_dims);
	}
	else{
	    working_image = out_int;
	}
	
	compute_NFFTH_NC2C( working_samples, working_image );
	
	if( !oversampled_image ){
	    crop<std::complex<T> ,D>( (matrix_size_os-matrix_size)>>1, working_image, out_int );
	}
	
	if( !oversampled_image ){
	    delete working_image; working_image = 0x0;
	}
	
	if( dcw_int ){
	    delete working_samples; working_samples = 0x0;
	}    
	break;
	
    case NFFT_BACKWARDS_C2NC:
	
	if( !oversampled_image ){
	    working_image = new hoNDArray<std::complex<T> >(&vec_dims);
	    
	    pad( matrix_size_os[1], in_int, working_image );
	}
	else{
	    working_image = in_int;
	}
	
	compute_NFFTH_C2NC( working_image, out_int );
	
	if( dcw_int )
	    multiply(*out_int, *dcw_int, *out_int);
        
	if( !oversampled_image ){
	    delete working_image; working_image = 0x0;
	}
	
	break;
    };
}



template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::mult_MH_M( hoNDArray<std::complex<T> > *in, hoNDArray<std::complex<T> > *out,
									       hoNDArray<T> *dcw, std::vector<size_t> halfway_dims )
{
    // Validity checks
    
    unsigned char components = _NFFT_CONV_C2NC + _NFFT_CONV_NC2C + _NFFT_FFT + _NFFT_DEAPODIZATION;
    
    if( in->get_number_of_elements() != out->get_number_of_elements() ){
	throw std::runtime_error("Error: hoNFFT_plan::mult_MH_M: in/out image sizes mismatch");
    }
    
    hoNDArray<std::complex<T> > *working_samples = new hoNDArray<std::complex<T> >(&halfway_dims);
    
    check_consistency( working_samples, in, dcw, components );
    
    hoNDArray<std::complex<T> > *in_int = 0x0;
    hoNDArray<std::complex<T> > *out_int = 0x0;
    hoNDArray<T> *dcw_int = 0x0;
    
    hoNDArray<std::complex<T> > *working_image = 0x0;
    
    typename uint64d<D>::Type image_dims = from_std_vector<size_t,D>(*in->get_dimensions()); 
    bool oversampled_image = (image_dims==matrix_size_os); 
    
    vector<size_t> vec_dims = to_std_vector(matrix_size_os); 
    for( unsigned int d=D; d<in->get_number_of_dimensions(); d++ )
	vec_dims.push_back(in->get_size(d));
    
    if( !oversampled_image ){
	working_image = new hoNDArray<std::complex<T> >(&vec_dims);
	pad( matrix_size_os[1], in_int, working_image );
    }
    else{
	working_image = in_int;
    }
    
    compute_NFFT_C2NC( working_image, working_samples );
    
    // Density compensation
    if( dcw ){
	multiply(*working_samples, *dcw_int, *working_samples);
	multiply(*working_samples, *dcw_int, *working_samples);
    }
    
    compute_NFFTH_NC2C( working_samples, working_image );
    
    delete working_samples;
    working_samples = 0x0;
    
    if( !oversampled_image ){
	crop<std::complex<T>, D>( (matrix_size_os-matrix_size)>>1, working_image, out_int );
	delete working_image; working_image = 0x0;
    }
    
    //restore<std::complex<T> ,std::complex<T> ,T>
    //	(old_device, out, out, out_int, in, in_int, dcw, dcw_int );    
}


template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::convolve( hoNDArray<std::complex<T> > *in, hoNDArray<std::complex<T> > *out,
									      hoNDArray<T> *dcw, NFFT_conv_mode mode, bool accumulate )
{
    unsigned char components;
    
    if( mode == NFFT_CONV_C2NC ) 
	components = _NFFT_CONV_C2NC;
    else
	components = _NFFT_CONV_NC2C;
    
    {
	hoNDArray<std::complex<T> > *samples, *image;
	
	if( mode == NFFT_CONV_C2NC ){
	    image = in; samples = out;
	} else{
	    image = out; samples = in;
	}
	
	check_consistency( samples, image, dcw, components );
    }
    
    hoNDArray<std::complex<T> > *in_int = 0x0, *out_int = 0x0;
    hoNDArray<T> *dcw_int = 0x0;
    
    //prepare<complext<REAL>, complext<REAL>, REAL>
    //	(device, &old_device, in, &in_int, out, &out_int, dcw, &dcw_int );
    
    hoNDArray<std::complex<T> > *working_samples = 0x0;
    
    typename uint64d<D>::Type image_dims = from_std_vector<size_t, D>
	(*(((mode == NFFT_CONV_C2NC) ? in : out )->get_dimensions())); 
    bool oversampled_image = (image_dims==matrix_size_os); 
    
    if( !oversampled_image ){
	throw std::runtime_error("Error: hoNFFT_plan::convolve: ERROR: oversampled image not provided as input.");
    }
    
    vector<size_t> vec_dims = to_std_vector(matrix_size_os); 
    {
	hoNDArray<std::complex<T> > *image = ((mode == NFFT_CONV_C2NC) ? in : out );
	for( unsigned int d=D; d<image->get_number_of_dimensions(); d++ )
	    vec_dims.push_back(image->get_size(d));
    }
    
    switch(mode){
	
    case NFFT_CONV_C2NC:
  	convolve_NFFT_C2NC( in_int, out_int, accumulate );
  	if( dcw_int ) multiply(*out_int, *dcw_int, *out_int);
	break;
	
    case NFFT_CONV_NC2C:
	
	// Density compensation
	if( dcw_int ){
	    working_samples = new hoNDArray<std::complex<T> >(*in_int);
	    multiply(*working_samples, *dcw_int, *working_samples);
	}
	else{
	    working_samples = in_int;
	}
	
	convolve_NFFT_NC2C( working_samples, out_int, accumulate );
	
	if( dcw_int ){
	    delete working_samples; working_samples = 0x0;
	}    
	break;
	
    default:
	throw std::runtime_error( "Error: hoNFFT_plan::convolve: unknown mode.");
    }
    
    //restore<complext<REAL>, complext<REAL>, REAL>
    //	(old_device, out, out, out_int, in, in_int, dcw, dcw_int );
}


template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::fft(hoNDArray<std::complex<T> > *data, NFFT_fft_mode mode, bool do_scale )
{
    hoNDArray<std::complex<T> > *data_int = 0x0;
        
    //typename uint64d<D>::Type _dims_to_transform = counting_vec<size_t,D>();
    //vector<size_t> dims_to_transform = to_std_vector( _dims_to_transform );
    
    if( mode == NFFT_FORWARDS ){
	for(int i=0; i<D; i++){
	    hoNDFFT<T>::instance()->fft( data_int, i );
	}
    }
    else{
	for(int i=0; i<D; i++){
	    hoNDFFT<T>::instance()->ifft( data_int, i );
	}
    }
}

template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::deapodize( hoNDArray<std::complex<T> > *image, bool fourier_domain)
{
    unsigned char components;
    components = _NFFT_FFT;
    check_consistency( 0x0, image, 0x0, components );
    
    hoNDArray<std::complex<T> > *image_int = 0x0;
    
    typename uint64d<D>::Type image_dims = from_std_vector<size_t, D>(*image->get_dimensions()); 
    bool oversampled_image = (image_dims==matrix_size_os); 
    
    if( !oversampled_image ){
	throw std::runtime_error( "Error: hoNFFT_plan::deapodize: ERROR: oversampled image not provided as input.");
    }
    
    if (fourier_domain){
  	if (!deapodization_filterFFT)
	    deapodization_filterFFT = compute_deapodization_filter(true);
	multiply(*image_int, *deapodization_filterFFT, *image_int);
    } 
    else {
  	if (!deapodization_filter)
	    deapodization_filter = compute_deapodization_filter(false);
	multiply(*image_int, *deapodization_filter, *image_int);
    }
 }

//
// Private class methods
//

template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::check_consistency( hoNDArray<std::complex<T> > *samples, hoNDArray<std::complex<T> > *image,
										       hoNDArray<T> *weights, unsigned char components )
{

    if( !initialized ){
	throw std::runtime_error( "Error: hoNFFT_plan: Unable to proceed without setup.");
    }
    
    if( (components & _NFFT_CONV_C2NC ) && !preprocessed_C2NC ){
	throw std::runtime_error("Error: hoNFFT_plan: Unable to compute NFFT before preprocessing.");
    }
    
    if( (components & _NFFT_CONV_NC2C ) && !preprocessed_NC2C ){
	throw std::runtime_error("Error: hoNFFT_plan: Unable to compute NFFT before preprocessing.");
    }
    
    if( ((components & _NFFT_CONV_C2NC ) || (components & _NFFT_CONV_NC2C )) && !(image && samples) ){
	throw std::runtime_error("Error: hoNFFT_plan: Unable to process 0x0 input/output.");
    }
    
    if( ((components & _NFFT_FFT) || (components & _NFFT_DEAPODIZATION )) && !image ){
	throw std::runtime_error("Error: hoNFFT_plan: Unable to process 0x0 input.");
    }
    
    if( image->get_number_of_dimensions() < D ){
	throw std::runtime_error("Error: hoNFFT_plan: Number of image dimensions mismatch the plan.");
    }    
    
    typename uint64d<D>::Type image_dims = from_std_vector<size_t,D>( *image->get_dimensions() );
    bool oversampled_image = (image_dims==matrix_size_os);
    
    if( !((oversampled_image) ? (image_dims == matrix_size_os) : (image_dims == matrix_size) )){
	throw std::runtime_error("Error: hoNFFT_plan: Image dimensions mismatch.");
    }
    
    if( (components & _NFFT_CONV_C2NC ) || (components & _NFFT_CONV_NC2C )){    
	if( (samples->get_number_of_elements() == 0) || (samples->get_number_of_elements() % (number_of_frames*number_of_samples)) ){
	    printf("\nhoNFFT::check_consistency() failed:\n#elements in the samples array: %ld.\n#samples from preprocessing: %d.\n#frames from preprocessing: %d.\n",samples->get_number_of_elements(), number_of_samples, number_of_frames ); fflush(stdout);
	    throw std::runtime_error("Error: hoNFFT_plan: The number of samples is not a multiple of #samples/frame x #frames as requested through preprocessing");
	}
    
	unsigned int num_batches_in_samples_array = samples->get_number_of_elements()/(number_of_frames*number_of_samples);
	unsigned int num_batches_in_image_array = 1;
	
	for( unsigned int d=D; d<image->get_number_of_dimensions(); d++ ){
	    num_batches_in_image_array *= image->get_size(d);
	}
	num_batches_in_image_array /= number_of_frames;
	
	if( num_batches_in_samples_array != num_batches_in_image_array ){
	    printf("\nhoNFFT::check_consistency() failed:\n#elements in the samples array: %ld.\n#samples from preprocessing: %d.\n#frames from preprocessing: %d.\nLeading to %d batches in the samples array.\nThe number of batches in the image array is %d.\n",samples->get_number_of_elements(), number_of_samples, number_of_frames, num_batches_in_samples_array, num_batches_in_image_array ); fflush(stdout);
	    throw std::runtime_error("Error: hoNFFT_plan: Number of batches mismatch between samples and image arrays");
	}
    }
    
    if( components & _NFFT_CONV_NC2C ){
	if( weights ){ 
	    if( weights->get_number_of_elements() == 0 ||
		!( weights->get_number_of_elements() == number_of_samples || 
		   weights->get_number_of_elements() == number_of_frames*number_of_samples) ){
		printf("\ncuNFFT::check_consistency() failed:\n#elements in the samples array: %ld.\n#samples from preprocessing: %d.\n#frames from preprocessing: %d.\n#weights: %ld.\n",samples->get_number_of_elements(), number_of_samples, number_of_frames, weights->get_number_of_elements() ); fflush(stdout);
		throw std::runtime_error("Error: hoNFFT_plan: The number of weights should match #samples/frame x #frames as requested through preprocessing");
	    }
	}
    }  
}



template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::barebones()
{	
    // These are the fundamental booleans checked before accessing the various member pointers
    initialized = preprocessed_C2NC = preprocessed_NC2C = false;
    
    // Clear matrix sizes
    clear(matrix_size);
    clear(matrix_size_os);
    
    // Clear pointers
    trajectory_positions = 0x0;
    //tuples_last = bucket_begin = bucket_end = 0x0;

    //KaiserBessel_operators<T> kb;
   
}


template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::wipe( NFFT_wipe_mode mode )
{
    
    if( mode==NFFT_WIPE_ALL && initialized ){
	deapodization_filter.reset();
	initialized = false;
    }
    
    //if( preprocessed_NC2C ){
    //if( tuples_last )  delete tuples_last;
    //if( bucket_begin ) delete bucket_begin;
    //if( bucket_end )   delete bucket_end;
    //}
  
    if( preprocessed_C2NC || preprocessed_NC2C ){
    delete trajectory_positions;
    preprocessed_C2NC = preprocessed_NC2C = false;
    }
}



template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::compute_beta()
{	
    // Compute Kaiser-Bessel beta paramter according to the formula provided in 
    // Beatty et. al. IEEE TMI 2005;24(6):799-808.
    // Alpha is the oversampling factor and beta is the convolution kernel control parameter. Both in all three dimensions. 
    for( unsigned int d=0; d<D; d++ )
	beta[d] = (M_PI*std::sqrt((W*W)/(alpha[d]*alpha[d])*(alpha[d]-T(0.5))*(alpha[d]-T(0.5))-T(0.8))); 
}



//
// Function to calculate the deapodization filter
//

template<class T, unsigned int D> boost::shared_ptr<hoNDArray<std::complex<T> > > Gadgetron::hoNFFT_plan<T,D>::compute_deapodization_filter( bool FFTed)
{
    KaiserBessel_operators<T> kb;

    std::vector<size_t> tmp_vec_os = to_std_vector(matrix_size_os);
    
    boost::shared_ptr< hoNDArray<std::complex<T> > > filter( new hoNDArray<std::complex<T> >(tmp_vec_os));
    vector_td<T,D> matrix_size_os_real = vector_td<T,D>(matrix_size_os);
    
    // Find dimensions of grid/blocks.
    //dim3 dimBlock( 256 ); // 256 x 1 x 1
    //dim3 dimGrid( (prod(matrix_size_os)+dimBlock.x-1)/dimBlock.x ); 

    
    const unsigned int num_elements = prod(matrix_size_os);

    // Output weight
    //std::complex<T>  result;    
    
    for(int i = 0; i < num_elements; i++){
	typename uintd<D>::Type cell_pos;
	int i_tmp = i;
	for(int d = 0; d < D; d++){
	    cell_pos[d] = i_tmp % matrix_size_os[d];
	    i_tmp -= cell_pos[d];
	    i_tmp /= matrix_size_os[d];
	}
	//= idx_to_co<D>(i, matrix_size_os);
	
	
	// Sample position ("origin")
	const vector_td<T,D> sample_pos = T(0.5)*matrix_size_os_real;
	
	// Calculate the distance between the cell and the sample
	vector_td<T,D> cell_pos_real = vector_td<T,D>(cell_pos);
	const typename reald<T,D>::Type delta = abs(sample_pos-cell_pos_real);
	
	// Compute convolution weight. 
	T weight; 
	T zero = T(0);
	T half_W = T(0.5)*W; //half kernel width
	T one_over_W = T(1)/W;
	vector_td<T,D> half_W_vec( half_W );
	
	
	if( weak_greater( delta, half_W_vec ) )
	    weight = zero;
	else{ 
	    weight = kb.KaiserBessel( delta, matrix_size_os_real, one_over_W, beta );
	    //if( !isfinite(weight) )
	    //weight = zero;
	}

	//result.vec[0] = weight; 
	//result.vec[1] = zero;
	filter->get_data_ptr()[i] = (weight, zero);
    }
    
    
    // FFT
    if (FFTed)
  	fft( filter.get(), NFFT_FORWARDS, false );
    else
  	fft( filter.get(), NFFT_BACKWARDS, false );
    // Reciprocal
    reciprocal_inplace(filter.get());
    return filter;
 }



template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::compute_NFFT_C2NC( hoNDArray<std::complex<T> > *image, hoNDArray<std::complex<T> > *samples )
{
    // private method - no consistency check. We trust in ourselves.
    
    // Deapodization
    deapodize( image );
    
    // FFT
    fft( image, NFFT_FORWARDS );
    
    // Convolution
    convolve( image, samples, 0x0, NFFT_CONV_C2NC );
 }

template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::compute_NFFTH_NC2C( hoNDArray<std::complex<T> > *samples, hoNDArray<std::complex<T> > *image )
{
    // private method - no consistency check. We trust in ourselves.
    
    // Convolution
    convolve( samples, image, 0x0, NFFT_CONV_NC2C );
    
    // FFT
    fft( image, NFFT_BACKWARDS );
    
    // Deapodization  
    deapodize( image );
}

template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::compute_NFFTH_C2NC( hoNDArray<std::complex<T> > *image, hoNDArray<std::complex<T> > *samples )
{
    // private method - no consistency check. We trust in ourselves.
    
    // Deapodization
    deapodize( image, true );
    
    // FFT
    fft( image, NFFT_BACKWARDS );
    
    // Convolution
    convolve( image, samples, 0x0, NFFT_CONV_C2NC );
}

template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::compute_NFFT_NC2C( hoNDArray<std::complex<T> > *samples, hoNDArray<std::complex<T> > *image )
{
    // private method - no consistency check. We trust in ourselves.
    
    // Convolution
    convolve( samples, image, 0x0, NFFT_CONV_NC2C );
    
    // FFT
    fft( image, NFFT_FORWARDS );
    
    // Deapodization
    deapodize( image, true );
}



template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::convolve_NFFT_C2NC( hoNDArray<std::complex<T> > *image, hoNDArray<std::complex<T> > *samples, bool accumulate )
{
    // private method - no consistency check. We trust in ourselves.
    
    unsigned int num_batches = 1;
    for( unsigned int d=D; d<image->get_number_of_dimensions(); d++ )
	num_batches *= image->get_size(d);
    num_batches /= number_of_frames; // Number of samples/number of frames
    
    
    
    //Setup grid and threads
    
    
    //size_t threads_per_block;
    //unsigned int max_coils;
    
    //threads_per_block = NFFT_THREADS_PER_KERNEL;
    
    /**if( cudaDeviceManager::Instance()->major_version(device) == 1 ){
	max_coils = NFFT_MAX_COILS_COMPUTE_1x;
    }
    else{
	max_coils = NFFT_MAX_COILS_COMPUTE_2x;
    }*/
  
    // We can (only) convolve max_coils batches per run due to shared memory issues. 
    //unsigned int domain_size_coils_desired = num_batches;
    //unsigned int num_repetitions = domain_size_coils_desired/max_coils + 
    //( ((domain_size_coils_desired%max_coils)==0) ? 0 : 1 );
    //unsigned int domain_size_coils = (num_repetitions==1) ? domain_size_coils_desired : max_coils;
    //unsigned int domain_size_coils_tail = (num_repetitions==1) ? domain_size_coils_desired : domain_size_coils_desired - (num_repetitions-1)*domain_size_coils;
    
    // Block and Grid dimensions
    //dim3 dimBlock( (unsigned int)threads_per_block );
    //dim3 dimGrid( (number_of_samples+dimBlock.x-1)/dimBlock.x, number_of_frames );
    
    // Calculate how much shared memory to use per thread
    //size_t bytes_per_thread = domain_size_coils * sizeof( vector_td<REAL,D> );
    //size_t bytes_per_thread_tail = domain_size_coils_tail * sizeof( vector_td<REAL,D> );
    
    //unsigned int double_warp_size_power=0;
    //unsigned int __tmp = cudaDeviceManager::Instance()->warp_size(device)<<1;
    //while(__tmp!=1){
    //	__tmp>>=1;
    //	double_warp_size_power++;
    //}
    
    vector_td<T,D> matrix_size_os_real = vector_td<T,D>( matrix_size_os );
    
    
    //Invoke kernel
    
    /**
    for( unsigned int repetition = 0; repetition<num_repetitions; repetition++ ){
	NFFT_convolve_kernel<REAL,D>
	    <<<dimGrid, dimBlock, ((repetition==num_repetitions-1) ? dimBlock.x*bytes_per_thread_tail : dimBlock.x*bytes_per_thread)>>>
	    ( alpha, beta, W, vector_td<unsigned int,D>(matrix_size_os), vector_td<unsigned int,D>(matrix_size_wrap), number_of_samples,
	      (repetition==num_repetitions-1) ? domain_size_coils_tail : domain_size_coils, 
	      raw_pointer_cast(&(*trajectory_positions)[0]), 
	      image->get_data_ptr()+repetition*prod(matrix_size_os)*number_of_frames*domain_size_coils,
	      samples->get_data_ptr()+repetition*number_of_samples*number_of_frames*domain_size_coils, 
	      double_warp_size_power, REAL(0.5)*W, REAL(1)/(W), accumulate, matrix_size_os_real );
	
	CHECK_FOR_CUDA_ERROR();    
    } */
}

template<class T, unsigned int D> void Gadgetron::hoNFFT_plan<T,D>::convolve_NFFT_NC2C( hoNDArray<std::complex<T> > *image, hoNDArray<std::complex<T> > *samples, bool accumulate )
{
    
    KaiserBessel_operators<T> kb;

    /**
    // Bring in some variables from the plan
    
    //unsigned int device = plan->device;
    unsigned int number_of_frames = plan->number_of_frames;
    unsigned int number_of_samples = plan->number_of_samples;
    typename uint64d<D>::Type matrix_size_os = plan->matrix_size_os;
    typename uint64d<D>::Type matrix_size_wrap = plan->matrix_size_wrap;
    typename reald<T,D>::Type alpha = plan->alpha;
    typename reald<T,D>::Type beta = plan->beta;
    T W = plan->W;
    thrust::device_vector< typename reald<T,D>::Type > *trajectory_positions = plan->trajectory_positions;    
    //thrust::device_vector<unsigned int> *tuples_last = plan->tuples_last;
    //thrust::device_vector<unsigned int> *bucket_begin = plan->bucket_begin;
    //thrust::device_vector<unsigned int> *bucket_end = plan->bucket_end;
    */
    
    unsigned int num_batches = 1;
    for( unsigned int d=D; d<image->get_number_of_dimensions(); d++ )
	num_batches *= image->get_size(d);
    num_batches /= number_of_frames; //E.g. for receive coils. Then batches becomes number of coils. 
    
    /**
    //
    // Setup grid and threads
    //
    
    size_t threads_per_block;
    unsigned int max_coils;
    
    //threads_per_block = NFFT_THREADS_PER_KERNEL; //192
    */
    /**
    if( cudaDeviceManager::Instance()->major_version(device) == 1 ){
	max_coils = NFFT_MAX_COILS_COMPUTE_1x; //8
    }
    else{
	max_coils = NFFT_MAX_COILS_COMPUTE_2x; //16
    }
    */

    /**
    // We can (only) convolve domain_size_coils batches per run due to shared memory issues. 
    unsigned int domain_size_coils_desired = num_batches; //E.g. number of receive coils
    unsigned int num_repetitions = domain_size_coils_desired/max_coils + 
	( ((domain_size_coils_desired%max_coils)==0) ? 0 : 1 ); 
    unsigned int domain_size_coils = (num_repetitions==1) ? domain_size_coils_desired : max_coils; 
    //The length of the last repetition is thus:
    unsigned int domain_size_coils_tail = (num_repetitions==1) ? domain_size_coils_desired : domain_size_coils_desired - (num_repetitions-1)*domain_size_coils; 
    
    // Block and Grid dimensions
    dim3 dimBlock( (unsigned int)threads_per_block ); 
    dim3 dimGrid( (prod(matrix_size_os+matrix_size_wrap)+dimBlock.x-1)/dimBlock.x, number_of_frames );
    
    // Calculate how much shared memory to use per thread
    size_t bytes_per_thread = domain_size_coils * sizeof( vector_td<REAL,D> );
    size_t bytes_per_thread_tail = domain_size_coils_tail * sizeof( vector_td<REAL,D> );
    
    unsigned int double_warp_size_power=0, __tmp = cudaDeviceManager::Instance()->warp_size(device)<<1;
    while(__tmp!=1){
	__tmp>>=1;
	double_warp_size_power++;
    }
    */
    vector_td<T,D> matrix_size_os_real = vector_td<T,D>( matrix_size_os );
    
    // Define temporary image that includes a wrapping zone
    hoNDArray<std::complex<T> > _tmp;
    
    vector<size_t> vec_dims = to_std_vector(matrix_size_os+matrix_size_wrap); //Size of the image, including the expansion due to the size of convolution kernel 
    if( number_of_frames > 1 )
	vec_dims.push_back(number_of_frames);
    if( num_batches > 1 ) //E.g. if more receive coils
	vec_dims.push_back(num_batches); //Adds an element to vec_dim which has the value of Num_batches. E.e. [128 128 128] with 30 receive coils gives [128 128 128 30]
    
    _tmp.create(&vec_dims); //Image dimensions
    
    //
    // Invoke kernel
    //
    
    /**
    for( unsigned int repetition = 0; repetition<num_repetitions; repetition++ ){ //For 
	
	NFFT_H_convolve_kernel<REAL,D>
	    <<<dimGrid, dimBlock, ((repetition==num_repetitions-1) ? dimBlock.x*bytes_per_thread_tail : dimBlock.x*bytes_per_thread)>>>
	    (    alpha, //Over sampling factor #
	         beta,  //Convolution kernel control parameter #
	         W, //Kernel width #
	         vector_td<unsigned int,D>(matrix_size_os+matrix_size_wrap), //domainPos # (domain_count_grid)
	         number_of_samples, //Number of non-cartesian samples including number of receive coils, not including number_of_repetitions # (number_of_samples)
		 (repetition==num_repetitions-1) ? domain_size_coils_tail : domain_size_coils, //# (number_of_batches)
	         raw_pointer_cast(&(*trajectory_positions)[0]), //Trajectory positions # (trajectory_positions)
		 _tmp.get_data_ptr()+repetition*prod(matrix_size_os+matrix_size_wrap)*number_of_frames*domain_size_coils, //# (image)
	         samples->get_data_ptr()+repetition*number_of_samples*number_of_frames*domain_size_coils, //Samples: Positions the data pointer # (samples)
	         raw_pointer_cast(&(*tuples_last)[0]), //# (tubles_last)
	         raw_pointer_cast(&(*bucket_begin)[0]), //# (bucket_begin)
	         raw_pointer_cast(&(*bucket_end)[0]), //# (bucket_end)
	         double_warp_size_power, //# (double_warp_size_power)
	         REAL(0.5)*W, //Half of kernel size # (half_W)
	         REAL(1)/(W), //One over kernel size # (one_over_W)
	         matrix_size_os_real ); //Oversampling matrix size # (matrix_size_os_real)
    }
    
    */
    
    vector_td<unsigned int,D> domain_count_grid = vector_td<unsigned int,D>(matrix_size_os+matrix_size_wrap);
    const unsigned int number_of_domains = prod(domain_count_grid); //Sum of all elements, im dims os and wrap and for all receive coils
    
    vector_td<T,D> domainPos = vector_td<T,D>(matrix_size_os + matrix_size_wrap); //Added the kernel width to account for the convolving at the edges

    // Cell position as reald
    //vector_td<T,D> cell_pos = vector_td<T,D>( domainPos );
  
    /**
    // Convolve samples onto the domain (shared memory)
    //const unsigned int frame_offset = blockIdx.y*number_of_domains;
    //for( unsigned int i=bucket_begin[globalThreadId+frame_offset]; i<bucket_end[globalThreadId+frame_offset]; i++ ){
    for( int i = 0; i < traj_positions.get_number_of_elements(); i++ ){
	
	const unsigned int domainIdx = i;
	const vector_td<unsigned int,D> domainPos;
	int i_tmp = i;
	for(int d = 0; d < D; d++){
	    domain_pos[d] = i_tmp % domain_count_grid[d];
	    i_tmp -= domainPos[d];
	    i_tmp /= domain_count_grid[d];
	}

	// Cell position as reald
	vector_td<T,D> cell_pos = vector_td<T,D>( domainPos );

	const unsigned int num_reals = number_of_batches<<1; //E.g. number of receive coils * 2

	// Safety precaution TODO
	//unsigned int sampleIdx = tuples_last[i];
	
	// Safety precaution TODO
	vector_td<T,D> sample_pos = ( traj_positions[i] + T(0.5) ) * matrix_size_os; //Putting the value in the rance [0 ; matrix_size_os]
	
	// Calculate the distance between the cell and the sample
	vector_td<T,D> delta = abs(sample_pos-cell_pos); //if mat_s_os=256, kernel_width=4, then delta is in the interval [4 ; 258]
	vector_td<T,D> half_W_vec( half_W ); //From above example this will be a vector of D entries all with value 4/2=2
	
	// Check if sample will contribute
	if( weak_greater(delta, half_W_vec )) //Then the following is only run if the sample is within the kernel position
	    continue;
	
	// Compute convolution weights
	T weight = kb.KaiserBessel( delta, matrix_size_os_real, one_over_W, beta );
	
	// Safety measure
	if( !isfinite(weight) )
	    continue;
	
	// Apply Kaiser-Bessel filter to input images
	for( unsigned int batch=0; batch<number_of_batches; batch++ ){
	    
	    complext<T>sample_val = samples[sampleIdx+batch*gridDim.y*number_of_samples];
	    
	    // Apply filter to shared memory domain. 
	    shared_mem[sharedMemFirstCellIdx+(batch<<double_warp_size_power)] += (weight*sample_val.vec[0]);
	    shared_mem[sharedMemFirstCellIdx+(batch<<double_warp_size_power)+warpSize] += (weight*sample_val.vec[1]);
	}
    }
    
    plan->image_wrap( &_tmp, image, accumulate );
    //};
*/};

/**
// Image wrap kernels

template<class REAL, unsigned int D> __global__ void
image_wrap_kernel( typename uintd<D>::Type matrix_size_os, typename uintd<D>::Type matrix_size_wrap, bool accumulate,
                   const complext<REAL> * __restrict__ in, complext<REAL> * __restrict__ out )
{
  unsigned int idx = blockIdx.x*blockDim.x + threadIdx.x;
  const unsigned int num_elements_per_image_src = prod(matrix_size_os+matrix_size_wrap);
  const unsigned int image_offset_src = blockIdx.y*num_elements_per_image_src;
  
  const typename uintd<D>::Type co = idx_to_co<D>(idx, matrix_size_os);
  const typename uintd<D>::Type half_wrap = matrix_size_wrap>>1;
  
  // Make "boolean" vectors denoting whether wrapping needs to be performed in a given direction (forwards/backwards)
  vector_td<bool,D> B_l = vector_less( co, half_wrap );
  vector_td<bool,D> B_r = vector_greater_equal( co, matrix_size_os-half_wrap );
  
  complext<REAL>  result = in[co_to_idx<D>(co+half_wrap, matrix_size_os+matrix_size_wrap) + image_offset_src];

  if( sum(B_l+B_r) > 0 ){
    
    // Fold back the wrapping zone onto the image ("periodically")
    //
    // There is 2^D-1 ways to pick combinations of dimensions in D-dimensionsal space, e.g. 
    // 
    //  { x, y, xy } in 2D
    //  { x, y, x, xy, xz, yz, xyz } in 3D
    //
    // Every "letter" in each combination provides two possible wraps (eiher end of the dimension)
    // 
    // For every 2^D-1 combinations DO
    //   - find the number of dimensions, d, in the combination
    //   - create 2^(d) stride vectors and test for wrapping using the 'B'-vectors above.
    //   - accumulate the contributions
    // 
    //   The following code represents dimensions as bits in a char.
    //
    
    for( unsigned char combination = 1; combination < (1<<D); combination++ ){
    
      // Find d
      unsigned char d = 0;
      for( unsigned char i=0; i<D; i++ )
        d += ((combination & (1<<i)) > 0 );
       
      // Create stride vector for each wrapping test
      for( unsigned char s = 0; s < (1<<d); s++ ){
        
        // Target for stride
        typename intd<D>::Type stride;
        char wrap_requests = 0;
        char skipped_dims = 0;
	
        // Fill dimensions of the stride
        for( unsigned char i=1; i<D+1; i++ ){
    
          // Is the stride dimension present in the current combination?
          if( i & combination ){
    
            // A zero bit in s indicates "check for left wrap" and a one bit is interpreted as "check for right wrap" 
            // ("left/right" for the individual dimension meaning wrapping on either side of the dimension).
    
            if( i & (s<<(skipped_dims)) ){
              if( B_r.vec[i-1] ){ // Wrapping required 
              	stride[i-1] = -1;
                wrap_requests++;
              }
              else
              	stride[i-1] = 0;
            }
            else{ 
              if( B_l.vec[i-1] ){ // Wrapping required 
              	stride[i-1] =1 ;
                wrap_requests++;
              }
              else
              	stride[i-1] = 0;
            }
          }
          else{
            // Do not test for wrapping in dimension 'i-1' (for this combination)
          	stride[i-1] = 0;
            skipped_dims++;
          }
        }
	
        // Now it is time to do the actual wrapping (if needed)
        if( wrap_requests == d ){
          typename intd<D>::Type src_co_int = vector_td<int,D>(co+half_wrap);
          typename intd<D>::Type matrix_size_os_int = vector_td<int,D>(matrix_size_os);
          typename intd<D>::Type co_offset_int = src_co_int + component_wise_mul<int,D>(stride,matrix_size_os_int);
          typename uintd<D>::Type co_offset = vector_td<unsigned int,D>(co_offset_int);
          result += in[co_to_idx<D>(co_offset, matrix_size_os+matrix_size_wrap) + image_offset_src];
          break; // only one stride per combination can contribute (e.g. one edge, one corner)
        } 
      } 
    }
  }
  
  // Output
  const unsigned int image_offset_tgt = blockIdx.y*prod(matrix_size_os);
  if( accumulate ) result += out[idx+image_offset_tgt];
  out[idx+image_offset_tgt] = result;
}

template<class REAL, unsigned int D, bool ATOMICS> void
Gadgetron::cuNFFT_plan<REAL,D,ATOMICS>::image_wrap( cuNDArray<complext<REAL> > *source, cuNDArray<complext<REAL> > *target, bool accumulate )
{
  unsigned int num_batches = 1;
  for( unsigned int d=D; d<source->get_number_of_dimensions(); d++ )
    num_batches *= source->get_size(d);
  num_batches /= number_of_frames;

  // Set dimensions of grid/blocks.
  unsigned int bdim = 256;
  dim3 dimBlock( bdim );
  dim3 dimGrid( prod(matrix_size_os)/bdim, number_of_frames*num_batches );

  // Safety check
  if( (prod(matrix_size_os)%bdim) != 0 ) {
  	std::stringstream ss;
  	ss << "Error: cuNFFT : the number of oversampled image elements must be a multiplum of the block size: " << bdim;
    throw std::runtime_error(ss.str());
  }

  // Invoke kernel
  image_wrap_kernel<REAL,D><<<dimGrid, dimBlock>>>
    ( vector_td<unsigned int,D>(matrix_size_os), vector_td<unsigned int,D>(matrix_size_wrap), accumulate, source->get_data_ptr(), target->get_data_ptr() );
  
  CHECK_FOR_CUDA_ERROR();
}	

//
// Template instantion
//

*/

template class hoNFFT_plan<float,  1>;
template class hoNFFT_plan<double, 1>;

template class hoNFFT_plan<float,  2>;
template class hoNFFT_plan<double, 2>;

template class hoNFFT_plan<float,  3>;
template class hoNFFT_plan<double, 3>;

template class hoNFFT_plan<float,  4>;
template class hoNFFT_plan<double, 4>;

